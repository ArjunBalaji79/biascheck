{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/zuse/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/anaconda3/envs/zuse/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias Score: 0.0\n",
      "Overall Sentiment: 0.12807219015552349\n",
      "\n",
      "Top Flagged Sentences:\n",
      "- Sentence: They say all the right things, and their resume is perfect for the job, except your gut tells you otherwise.\n",
      "  Similarity: 0.20\n",
      "  Sentiment: 0.64\n",
      "- Sentence: Leniency Bias: Occurs when the interviewer tends to go easy on a candidate and gives a higher rating than is warranted, justifying this with an explanation In some cases, individuals have a tendency to give the majority of candidates outstanding ratings across the board, ratings that are higher than they might be in reality.\n",
      "  Similarity: 0.23\n",
      "  Sentiment: 0.36\n",
      "- Sentence: Negative Emphasis Bias: Occurs when the interviewer allows a small amount of negative information to outweigh positive information Negative emphasis often happens when subjective factors like dress or nonverbal communication taint the interviewer's judgment.\n",
      "  Similarity: 0.30\n",
      "  Sentiment: -0.18\n",
      "- Sentence: Harshness Bias/Horn Effect: Occurs when the interviewer evaluates a candidate negatively based on a single characteristic In this case, the rater may have higher personal standards they’re comparing the candidates to, resulting in lower average ratings.\n",
      "  Similarity: 0.28\n",
      "  Sentiment: -0.06\n",
      "- Sentence: Gut Feeling Bias: Occurs when the interviewer relies on an intuitive feeling that the candidate is a good, or bad, fit for the position without looking as whether or not the individual’s qualifications meet the criteria established You’re interviewing an applicant for a job, and you feel something is off.\n",
      "  Similarity: 0.26\n",
      "  Sentiment: 0.10\n"
     ]
    }
   ],
   "source": [
    "from biascheck.analysis.docucheck import DocuCheck\n",
    "from biascheck.utils.terms_loader import load_document  \n",
    "\n",
    "\n",
    "document_path = \"/Users/balajis/Downloads/Bias.pdf\"  \n",
    "\n",
    "# Load and preprocess the document\n",
    "document_text = load_document(document_path)\n",
    "\n",
    "# Initialize DocuCheck\n",
    "analyzer = DocuCheck(\n",
    "    data=document_text,  # Pass the preprocessed text\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    bias_threshold=0.5,  # Tuned threshold for bias detection\n",
    "    use_ner=True,  # Enable Named Entity Recognition\n",
    "    verbose=False,  # Set to False for concise overview\n",
    ")\n",
    "\n",
    "# Analyze the document\n",
    "result = analyzer.analyze(top_n=5)\n",
    "\n",
    "# Display the results\n",
    "print(\"Bias Score:\", result[\"bias_score\"])\n",
    "print(\"Overall Sentiment:\", result[\"overall_sentiment\"])\n",
    "print(\"\\nTop Flagged Sentences:\")\n",
    "for item in result[\"top_flagged_sentences\"]:\n",
    "    print(f\"- Sentence: {item['sentence']}\\n  Similarity: {item['similarity']:.2f}\\n  Sentiment: {item['sentiment']:.2f}\")\n",
    "\n",
    "if result[\"flagged_entities\"]:\n",
    "    print(\"\\nFlagged Entities:\")\n",
    "    for entity, label in result[\"flagged_entities\"]:\n",
    "        print(f\"Entity: {entity} (Label: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zuse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
